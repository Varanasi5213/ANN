{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-20T10:00:27.145819Z","iopub.execute_input":"2021-08-20T10:00:27.146229Z","iopub.status.idle":"2021-08-20T10:00:27.152642Z","shell.execute_reply.started":"2021-08-20T10:00:27.146191Z","shell.execute_reply":"2021-08-20T10:00:27.151556Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Loading Libraries\nimport torch \nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torchvision import models\nfrom torch import optim\nfrom torch.autograd import Variable\nfrom torchvision.transforms import Compose\nfrom torchvision.datasets import CIFAR10","metadata":{"execution":{"iopub.status.busy":"2021-08-20T10:00:27.154513Z","iopub.execute_input":"2021-08-20T10:00:27.155318Z","iopub.status.idle":"2021-08-20T10:00:27.162488Z","shell.execute_reply.started":"2021-08-20T10:00:27.155259Z","shell.execute_reply":"2021-08-20T10:00:27.161629Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"device = \"cpu\"\n\nif torch.cuda.is_available():\n    device = \"cuda\"\n\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T10:00:27.213892Z","iopub.execute_input":"2021-08-20T10:00:27.214181Z","iopub.status.idle":"2021-08-20T10:00:27.219757Z","shell.execute_reply.started":"2021-08-20T10:00:27.214156Z","shell.execute_reply":"2021-08-20T10:00:27.218383Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"train_transform = Compose([\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomCrop(32, padding=4),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\ntest_transform = Compose([\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\ncifar10_train = CIFAR10(root = \"/data\", train=True, download = True, transform=train_transform)\ntrain_loader = torch.utils.data.DataLoader(cifar10_train, batch_size=64, shuffle=True)\n\ncifar10_test = CIFAR10(root = \"/data\", train=False, download = True, transform=test_transform)\ntest_loader = torch.utils.data.DataLoader(cifar10_test, batch_size=64, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T10:00:27.227474Z","iopub.execute_input":"2021-08-20T10:00:27.227851Z","iopub.status.idle":"2021-08-20T10:00:28.803252Z","shell.execute_reply.started":"2021-08-20T10:00:27.227814Z","shell.execute_reply":"2021-08-20T10:00:28.802391Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\n","output_type":"stream"}]},{"cell_type":"code","source":"vgg = models.vgg16(pretrained=True)\nvgg = vgg.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T10:00:28.805693Z","iopub.execute_input":"2021-08-20T10:00:28.806034Z","iopub.status.idle":"2021-08-20T10:00:30.343703Z","shell.execute_reply.started":"2021-08-20T10:00:28.806000Z","shell.execute_reply":"2021-08-20T10:00:30.342834Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"is_cuda = False\nif torch.cuda.is_available():\n    is_cuda = True","metadata":{"execution":{"iopub.status.busy":"2021-08-20T10:00:30.345224Z","iopub.execute_input":"2021-08-20T10:00:30.345629Z","iopub.status.idle":"2021-08-20T10:00:30.350334Z","shell.execute_reply.started":"2021-08-20T10:00:30.345590Z","shell.execute_reply":"2021-08-20T10:00:30.349471Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"vgg.classifier[6].out_features = 10\nfor param in vgg.features.parameters(): param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2021-08-20T10:00:30.351989Z","iopub.execute_input":"2021-08-20T10:00:30.352402Z","iopub.status.idle":"2021-08-20T10:00:30.360485Z","shell.execute_reply.started":"2021-08-20T10:00:30.352363Z","shell.execute_reply":"2021-08-20T10:00:30.359571Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"num_epochs = 5\nnum_classes = 10\nlearning_rate = 0.001","metadata":{"execution":{"iopub.status.busy":"2021-08-20T10:00:30.364076Z","iopub.execute_input":"2021-08-20T10:00:30.364510Z","iopub.status.idle":"2021-08-20T10:00:30.369428Z","shell.execute_reply.started":"2021-08-20T10:00:30.364475Z","shell.execute_reply":"2021-08-20T10:00:30.368209Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"optimizer = optim.SGD(vgg.classifier.parameters(),lr=0.005,momentum=0.5)\ncriterion = nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2021-08-20T10:00:30.371433Z","iopub.execute_input":"2021-08-20T10:00:30.371865Z","iopub.status.idle":"2021-08-20T10:00:30.379461Z","shell.execute_reply.started":"2021-08-20T10:00:30.371829Z","shell.execute_reply":"2021-08-20T10:00:30.378557Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"total_step = len(train_loader)\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        # Forward pass\n        outputs = vgg(images)\n        loss = criterion(outputs, labels)\n        \n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        if (i+1) % 100 == 0:\n            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n","metadata":{"execution":{"iopub.status.busy":"2021-08-20T10:00:30.381165Z","iopub.execute_input":"2021-08-20T10:00:30.381471Z","iopub.status.idle":"2021-08-20T10:02:20.516451Z","shell.execute_reply.started":"2021-08-20T10:00:30.381434Z","shell.execute_reply":"2021-08-20T10:02:20.515502Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Epoch [1/5], Step [100/782], Loss: 1.9152\nEpoch [1/5], Step [200/782], Loss: 1.2515\nEpoch [1/5], Step [300/782], Loss: 1.4755\nEpoch [1/5], Step [400/782], Loss: 1.3484\nEpoch [1/5], Step [500/782], Loss: 1.5471\nEpoch [1/5], Step [600/782], Loss: 1.3955\nEpoch [1/5], Step [700/782], Loss: 1.1922\nEpoch [2/5], Step [100/782], Loss: 1.4971\nEpoch [2/5], Step [200/782], Loss: 1.3904\nEpoch [2/5], Step [300/782], Loss: 1.2556\nEpoch [2/5], Step [400/782], Loss: 1.3125\nEpoch [2/5], Step [500/782], Loss: 1.4636\nEpoch [2/5], Step [600/782], Loss: 1.4264\nEpoch [2/5], Step [700/782], Loss: 1.2851\nEpoch [3/5], Step [100/782], Loss: 1.3450\nEpoch [3/5], Step [200/782], Loss: 1.3611\nEpoch [3/5], Step [300/782], Loss: 1.4913\nEpoch [3/5], Step [400/782], Loss: 1.0109\nEpoch [3/5], Step [500/782], Loss: 1.1550\nEpoch [3/5], Step [600/782], Loss: 1.4212\nEpoch [3/5], Step [700/782], Loss: 1.3468\nEpoch [4/5], Step [100/782], Loss: 1.2259\nEpoch [4/5], Step [200/782], Loss: 1.5160\nEpoch [4/5], Step [300/782], Loss: 1.2448\nEpoch [4/5], Step [400/782], Loss: 1.3271\nEpoch [4/5], Step [500/782], Loss: 1.3621\nEpoch [4/5], Step [600/782], Loss: 1.0728\nEpoch [4/5], Step [700/782], Loss: 1.1960\nEpoch [5/5], Step [100/782], Loss: 0.9830\nEpoch [5/5], Step [200/782], Loss: 1.4165\nEpoch [5/5], Step [300/782], Loss: 1.1586\nEpoch [5/5], Step [400/782], Loss: 1.2482\nEpoch [5/5], Step [500/782], Loss: 1.2378\nEpoch [5/5], Step [600/782], Loss: 1.0365\nEpoch [5/5], Step [700/782], Loss: 1.2243\n","output_type":"stream"}]},{"cell_type":"code","source":"# Test the model\nvgg.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\nwith torch.no_grad():\n    correct = 0\n    total = 0\n    for images, labels in test_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n        outputs = vgg(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n","metadata":{"execution":{"iopub.status.busy":"2021-08-20T10:02:20.517801Z","iopub.execute_input":"2021-08-20T10:02:20.518149Z","iopub.status.idle":"2021-08-20T10:02:24.596936Z","shell.execute_reply.started":"2021-08-20T10:02:20.518114Z","shell.execute_reply":"2021-08-20T10:02:24.595614Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Test Accuracy of the model on the 10000 test images: 60.9 %\n","output_type":"stream"}]}]}